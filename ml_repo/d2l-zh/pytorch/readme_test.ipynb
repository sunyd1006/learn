{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要测试 torch 的语法\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, 2) + (3, 4)  # (1, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position variables, \ta: 1\n",
      "default variables, \tb: 4\n",
      "variable position variables, \targs: (5, 6)\n",
      "keyword variables, \tc: 7\n",
      "variable kwargs, \tkargs: {'d': 8, 'e': 9}\n"
     ]
    }
   ],
   "source": [
    "def complex_function(a, b=2, *args, c=3, **kwargs):\n",
    "    print(f\"position variables, \\ta: {a}\")\n",
    "    print(f\"default variables, \\tb: {b}\")\n",
    "    print(f\"variable position variables, \\targs: {args}\")\n",
    "    print(f\"keyword variables, \\tc: {c}\")\n",
    "    print(f\"variable kwargs, \\tkargs: {kwargs}\")\n",
    "\n",
    "# 调用函数\n",
    "complex_function(1, 4, 5, 6, c=7, d=8, e=9)\n",
    "# 输出:\n",
    "# a: 1, b: 4\n",
    "# args: (5, 6)\n",
    "# c: 7\n",
    "# kwargs: {'d': 8, 'e': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a', True), (2, 'b', False), (3, 'c', True)]\n",
      "[(1, 'a'), (2, 'b')]\n",
      "[1, 2, 3]\n",
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "list1 = [1, 2, 3]\n",
    "list2 = ['a', 'b', 'c']\n",
    "# 多个可迭代对象\n",
    "list3 = [True, False, True]\n",
    "zipped = zip(list1, list2, list3)\n",
    "print(list(zipped))\n",
    "\n",
    "# 当传入的可迭代对象长度不一致时，zip 只会配对到最短的一个可迭代对象的长度，剩余的元素会被丢弃。\n",
    "list1 = [1, 2, 3]\n",
    "list2 = ['a', 'b']\n",
    "zipped = zip(list1, list2)\n",
    "print(list(zipped))\n",
    "\n",
    "# unzip\n",
    "zipped = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "# 解压\n",
    "unzipped = zip(*zipped)\n",
    "list1, list2 = map(list, unzipped)  # 使用 map 将解压结果转化为列表\n",
    "print(list1)  # [1, 2, 3]\n",
    "print(list2)  # ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch 语法总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个形状为 (1, 2, 3, 4) 的张量\n",
    "x = torch.rand(1, 2, 3, 4)\n",
    "print(x.shape)  # 输出: torch.Size([1, 2, 3, 4])\n",
    "x.reshape(2, 3, -1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Tensor along dim=0:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Stacked Tensor along dim=1:\n",
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n",
      "Stacked Tensor along dim=2:\n",
      "tensor([[[ 1,  5,  9],\n",
      "         [ 2,  6, 10]],\n",
      "\n",
      "        [[ 3,  7, 11],\n",
      "         [ 4,  8, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack: https://pytorch.org/docs/stable/generated/torch.stack.html\n",
    "\n",
    "# 使用 torch.stack 沿着第1维堆叠\n",
    "\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "stacked_1 = torch.stack([tensor1, tensor2, tensor3], dim=1)\n",
    "\n",
    "# 使用 torch.stack 沿着第0维堆叠\n",
    "stacked_0 = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
    "print(\"Stacked Tensor along dim=0:\")\n",
    "print(stacked_0)\n",
    "\n",
    "print(\"Stacked Tensor along dim=1:\")\n",
    "print(stacked_1)\n",
    "\n",
    "# 创建多个 2D 张量\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "tensor3 = torch.tensor([[9, 10], [11, 12]])\n",
    "# 使用 torch.stack 沿着第2维堆叠\n",
    "stacked_2 = torch.stack([tensor1, tensor2, tensor3], dim=2)\n",
    "print(\"Stacked Tensor along dim=2:\")\n",
    "print(stacked_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape torch.Size([2, 3, 3]) K.shape torch.Size([2, 2, 2])\n",
      "x, k =>  tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]]) tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "x, k =>  tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]) tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# zip torch\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0],\n",
    "                   [3.0, 4.0, 5.0],\n",
    "                   [6.0, 7.0, 8.0]],\n",
    "\n",
    "                  [[1.0, 2.0, 3.0],\n",
    "                   [4.0, 5.0, 6.0],\n",
    "                   [7.0, 8.0, 9.0]]])\n",
    "\n",
    "K = torch.tensor([[[0.0, 1.0],\n",
    "                   [2.0, 3.0]],\n",
    "\n",
    "                  [[1.0, 2.0],\n",
    "                   [3.0, 4.0]]])\n",
    "\n",
    "print(\"X.shape\", X.shape, \"K.shape\", K.shape)\n",
    "for x, k in zip(X, K):\n",
    "    print(\"x, k => \", x, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshape输入辨析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输入是reshape是到底是元组合适分开的参数\n",
    "\n",
    "# == 输入是元组的函数\n",
    "torchVar = torch.normal(mean=0.0, std=0.01, size=(12, 3))\n",
    "torchVar.reshape((-1, 2))\n",
    "_ = torch.zeros((100 - 4, 4)).shape    # torch.Size([96, 4])\n",
    "_ = torch.arange(6, 10).shape        # torch.Size([4])\n",
    "_ = torch.randn(size=(12, 3)).shape\n",
    "\n",
    "\n",
    "# d2l 相关的函数\n",
    "# d2l.load_array((features[:n_train], labels[:n_train])\n",
    "# _ = torch.tensor([0, 2])\n",
    "\n",
    "\n",
    "\n",
    "# == 输入是分开的参数\n",
    "_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1: \n",
      " tensor([[[0, 1]],\n",
      "\n",
      "        [[2, 3]],\n",
      "\n",
      "        [[4, 5]]]) \n",
      "b2: \n",
      " tensor([[[ 7,  8]],\n",
      "\n",
      "        [[ 9, 10]],\n",
      "\n",
      "        [[11, 12]]])\n",
      "unsqueeze(0):  tensor([[[[0, 1]],\n",
      "\n",
      "         [[2, 3]],\n",
      "\n",
      "         [[4, 5]]]])\n",
      "Shape of result: \n",
      " torch.Size([2, 3, 1, 2])\n",
      "tensor([[[[4.]],\n",
      "\n",
      "         [[6.]],\n",
      "\n",
      "         [[8.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建 b1 和 b2\n",
    "b1 = torch.arange(6).reshape(3, 1, 2)\n",
    "b2 = torch.arange(7, 13).reshape(3, 1, 2)\n",
    "\n",
    "# 打印 b1 和 b2\n",
    "print(\"b1: \\n\", b1, \"\\nb2: \\n\", b2)\n",
    "# unsqueeze(0)：将 b1 和 b2 在第 0 维上扩展成形状为 (1, 3, 1, 2)，这样 torch.cat 才能在第 0 维上拼接。\n",
    "print(\"unsqueeze(0): \", b1.unsqueeze(0))\n",
    "\n",
    "# 使用 torch.cat 拼接沿第 0 维, 输入一个(2, 3, 1, 2)的矩阵\n",
    "result = torch.cat((b1.unsqueeze(0), b2.unsqueeze(0)), dim=0)\n",
    "print(\"Shape of result: \\n\", result.shape)\n",
    "\n",
    "# 总结：dim中没有带数字1，即保留下channels维度其他的求和 => (1, 3, 1, 1)\n",
    "# (0 + 1 + 7 + 8) / 4 = 4;        (2 + 3 + 9 + 10) / 4 = 16\n",
    "print(result.mean(dim=(0, 2, 3), keepdim=True, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.mean"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
