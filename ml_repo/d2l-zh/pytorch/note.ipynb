{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主要测试 torch 的语法\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "索引的开闭: \n",
    "* random.randint(a, b) 返回的是[a, b],\n",
    "* 其他的注入torch[a: b] 是[a, b), range(start, end, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1, 2) + (3, 4)  # (1, 2, 3, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position variables, \ta: 1\n",
      "default variables, \tb: 4\n",
      "variable position variables, \targs: (5, 6)\n",
      "keyword variables, \tc: 7\n",
      "variable kwargs, \tkargs: {'d': 8, 'e': 9}\n"
     ]
    }
   ],
   "source": [
    "def complex_function(a, b=2, *args, c=3, **kwargs):\n",
    "    print(f\"position variables, \\ta: {a}\")\n",
    "    print(f\"default variables, \\tb: {b}\")\n",
    "    print(f\"variable position variables, \\targs: {args}\")\n",
    "    print(f\"keyword variables, \\tc: {c}\")\n",
    "    print(f\"variable kwargs, \\tkargs: {kwargs}\")\n",
    "\n",
    "# 调用函数\n",
    "complex_function(1, 4, 5, 6, c=7, d=8, e=9)\n",
    "# 输出:\n",
    "# a: 1, b: 4\n",
    "# args: (5, 6)\n",
    "# c: 7\n",
    "# kwargs: {'d': 8, 'e': 9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'a', True), (2, 'b', False), (3, 'c', True)]\n",
      "[(1, 'a'), (2, 'b')]\n",
      "[1, 2, 3]\n",
      "['a', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "list1 = [1, 2, 3]\n",
    "list2 = ['a', 'b', 'c']\n",
    "# 多个可迭代对象\n",
    "list3 = [True, False, True]\n",
    "zipped = zip(list1, list2, list3)\n",
    "print(list(zipped))\n",
    "\n",
    "# 当传入的可迭代对象长度不一致时，zip 只会配对到最短的一个可迭代对象的长度，剩余的元素会被丢弃。\n",
    "list1 = [1, 2, 3]\n",
    "list2 = ['a', 'b']\n",
    "zipped = zip(list1, list2)\n",
    "print(list(zipped))\n",
    "\n",
    "# unzip\n",
    "zipped = [(1, 'a'), (2, 'b'), (3, 'c')]\n",
    "# 解压\n",
    "unzipped = zip(*zipped)\n",
    "list1, list2 = map(list, unzipped)  # 使用 map 将解压结果转化为列表\n",
    "print(list1)  # [1, 2, 3]\n",
    "print(list2)  # ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch 语法总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS available: True\n",
      "MPS built: True\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "print(\"MPS available:\", torch.backends.mps.is_available())\n",
    "print(\"MPS built:\", torch.backends.mps.is_built())\n",
    "\n",
    "print(d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# 创建一个形状为 (1, 2, 3, 4) 的张量\n",
    "x = torch.rand(1, 2, 3, 4)\n",
    "print(x.shape)  # 输出: torch.Size([1, 2, 3, 4])\n",
    "x.reshape(2, 3, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 创建一个三维数组\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m Y \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m24\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m原始数组 Y 的形状:\u001b[39m\u001b[38;5;124m\"\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m原始数组 Y:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个三维数组\n",
    "Y = np.arange(24).reshape(2, 3, 4)\n",
    "print(\"原始数组 Y 的形状:\", Y.shape)\n",
    "print(\"原始数组 Y:\")\n",
    "print(Y)\n",
    "\n",
    "# 重塑数组\n",
    "reshaped_Y = Y.reshape((-1, Y.shape[-1]))\n",
    "print(\"\\n重塑后数组 reshaped_Y 的形状:\", reshaped_Y.shape)\n",
    "print(\"重塑后数组 reshaped_Y:\")\n",
    "print(reshaped_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Tensor along dim=0:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Stacked Tensor along dim=1:\n",
      "tensor([[1, 4, 7],\n",
      "        [2, 5, 8],\n",
      "        [3, 6, 9]])\n",
      "Stacked Tensor along dim=2:\n",
      "tensor([[[ 1,  5,  9],\n",
      "         [ 2,  6, 10]],\n",
      "\n",
      "        [[ 3,  7, 11],\n",
      "         [ 4,  8, 12]]])\n"
     ]
    }
   ],
   "source": [
    "# torch.stack: https://pytorch.org/docs/stable/generated/torch.stack.html\n",
    "\n",
    "# 使用 torch.stack 沿着第1维堆叠\n",
    "\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "tensor2 = torch.tensor([4, 5, 6])\n",
    "tensor3 = torch.tensor([7, 8, 9])\n",
    "stacked_1 = torch.stack([tensor1, tensor2, tensor3], dim=1)\n",
    "\n",
    "# 使用 torch.stack 沿着第0维堆叠\n",
    "stacked_0 = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
    "print(\"Stacked Tensor along dim=0:\")\n",
    "print(stacked_0)\n",
    "\n",
    "print(\"Stacked Tensor along dim=1:\")\n",
    "print(stacked_1)\n",
    "\n",
    "# 创建多个 2D 张量\n",
    "tensor1 = torch.tensor([[1, 2], [3, 4]])\n",
    "tensor2 = torch.tensor([[5, 6], [7, 8]])\n",
    "tensor3 = torch.tensor([[9, 10], [11, 12]])\n",
    "# 使用 torch.stack 沿着第2维堆叠\n",
    "stacked_2 = torch.stack([tensor1, tensor2, tensor3], dim=2)\n",
    "print(\"Stacked Tensor along dim=2:\")\n",
    "print(stacked_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape torch.Size([2, 3, 3]) K.shape torch.Size([2, 2, 2])\n",
      "x, k =>  tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]]) tensor([[0., 1.],\n",
      "        [2., 3.]])\n",
      "x, k =>  tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]]) tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "# zip torch\n",
    "X = torch.tensor([[[0.0, 1.0, 2.0],\n",
    "                   [3.0, 4.0, 5.0],\n",
    "                   [6.0, 7.0, 8.0]],\n",
    "\n",
    "                  [[1.0, 2.0, 3.0],\n",
    "                   [4.0, 5.0, 6.0],\n",
    "                   [7.0, 8.0, 9.0]]])\n",
    "\n",
    "K = torch.tensor([[[0.0, 1.0],\n",
    "                   [2.0, 3.0]],\n",
    "\n",
    "                  [[1.0, 2.0],\n",
    "                   [3.0, 4.0]]])\n",
    "\n",
    "print(\"X.shape\", X.shape, \"K.shape\", K.shape)\n",
    "for x, k in zip(X, K):\n",
    "    print(\"x, k => \", x, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reshape输入辨析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([24])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 输入是reshape是到底是元组合适分开的参数\n",
    "\n",
    "# == 输入是元组的函数\n",
    "torchVar = torch.normal(mean=0.0, std=0.01, size=(12, 3))\n",
    "# Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1\n",
    "_ = torch.randn(size=(12, 3)).shape\n",
    "torchVar.reshape((-1, 2))            # 重设形状为(?, 2)\n",
    "\n",
    "_ = torch.zeros((100 - 4, 4)).shape    # torch.Size([96, 4])\n",
    "\n",
    "_ = torch.arange(6, 10).shape        # torch.Size([4])\n",
    "_ = torch.arange(24)                 # torch.Size[24]\n",
    "\n",
    "\n",
    "\n",
    "# d2l 相关的函数\n",
    "# d2l.load_array((features[:n_train], labels[:n_train])\n",
    "# _ = torch.tensor([0, 2])\n",
    "\n",
    "\n",
    "\n",
    "# == 输入是分开的参数\n",
    "print(torchVar.shape)\n",
    "_.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1: \n",
      " tensor([[[0, 1]],\n",
      "\n",
      "        [[2, 3]],\n",
      "\n",
      "        [[4, 5]]]) \n",
      "b2: \n",
      " tensor([[[ 7,  8]],\n",
      "\n",
      "        [[ 9, 10]],\n",
      "\n",
      "        [[11, 12]]])\n",
      "unsqueeze(0):  tensor([[[[0, 1]],\n",
      "\n",
      "         [[2, 3]],\n",
      "\n",
      "         [[4, 5]]]])\n",
      "Shape of result: \n",
      " torch.Size([2, 3, 1, 2])\n",
      "tensor([[[[4.]],\n",
      "\n",
      "         [[6.]],\n",
      "\n",
      "         [[8.]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 创建 b1 和 b2\n",
    "b1 = torch.arange(6).reshape(3, 1, 2)\n",
    "b2 = torch.arange(7, 13).reshape(3, 1, 2)\n",
    "\n",
    "# 打印 b1 和 b2\n",
    "print(\"b1: \\n\", b1, \"\\nb2: \\n\", b2)\n",
    "# unsqueeze(0)：将 b1 和 b2 在第 0 维上扩展成形状为 (1, 3, 1, 2)，这样 torch.cat 才能在第 0 维上拼接。\n",
    "print(\"unsqueeze(0): \", b1.unsqueeze(0))\n",
    "\n",
    "# 使用 torch.cat 拼接沿第 0 维, 输入一个(2, 3, 1, 2)的矩阵\n",
    "result = torch.cat((b1.unsqueeze(0), b2.unsqueeze(0)), dim=0)\n",
    "print(\"Shape of result: \\n\", result.shape)\n",
    "\n",
    "# 总结：dim中没有带数字1，即保留下channels维度其他的求和 => (1, 3, 1, 1)\n",
    "# (0 + 1 + 7 + 8) / 4 = 4;        (2 + 3 + 9 + 10) / 4 = 16\n",
    "print(result.mean(dim=(0, 2, 3), keepdim=True, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.unsqueeze\n",
    "https://docs.pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch-unsqueeze\n",
    "\n",
    "torch.unsqueeze(input, dim) 就在在dim处加入一个空维度。 其中dim范围[-(input.dim+1), dim]； 即当dim为负数时，dim = dim + input.dim() + 1。例如imput.dim()=2, -3 = 0, -2 = 1, -1 = 2; 是一一对应的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3, 4]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "\n",
    "print(x.unsqueeze(0))\n",
    "# tensor([[1, 2, 3, 4]])\n",
    "\n",
    "torch.unsqueeze(x, 0)\n",
    "# tensor([[1, 2, 3, 4]])\n",
    "\n",
    "torch.unsqueeze(x, 0).unsqueeze(0)\n",
    "# tensor([[[1, 2, 3, 4]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.repeat\n",
    "\n",
    "torch.repeat() 按指定次数沿每个维度复制（tile）张量的数据。\n",
    "\n",
    "torch.repeat_interleave(input, repeats, dim=None) 是 逐元素重复（element-wise repeat），而 repeat 是 逐维度重复（tile repeat）。\n",
    "- input: 输入张量。\n",
    "- repeats: 每个元素重复的次数，可以是：\n",
    "- 一个整数（所有元素重复相同次数）；\n",
    "- 一个与 input.shape[dim] 相同的 1D 张量（每个元素重复不同次数）。\n",
    "- dim: 沿哪个维度重复（默认是扁平化后再重复）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 1, 2, 3])\n",
      "tensor([[1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4],\n",
      "        [1, 2, 1, 2, 1, 2],\n",
      "        [3, 4, 3, 4, 3, 4]])\n",
      "tensor([[1, 2, 3, 1, 2, 3, 1, 2, 3],\n",
      "        [1, 2, 3, 1, 2, 3, 1, 2, 3]])\n",
      "(2,1) tensor([[1, 2, 3],\n",
      "        [1, 2, 3]])\n",
      "tensor([1, 1, 2, 2, 3, 3])\n",
      "tensor([[1, 2],\n",
      "        [1, 2],\n",
      "        [3, 4],\n",
      "        [3, 4]])\n",
      "tensor([10, 20, 20, 30, 30, 30])\n",
      "tensor([[1, 1, 2, 2],\n",
      "        [3, 3, 4, 4]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3])  # shape: (3,)\n",
    "out = x.repeat(2)  # 重复整个 x 两次\n",
    "print(out)  # tensor([1, 2, 3, 1, 2, 3])\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]])  # shape: (2, 2)\n",
    "out = x.repeat(2, 3)  # 沿第0维重复2次，沿第1维重复3次\n",
    "print(out)\n",
    "\n",
    "x = torch.tensor([1, 2, 3])  # shape: (3,)\n",
    "out = x.repeat(2, 3)  # 因为原始张量 x 是一维的 (3,)，而 .repeat() 需要两个维度的信息 (2, 3)，所以 PyTorch 会自动在最前面添加一个维度，使其变为 (1, 3)。这一步称为“自动左补 1”。\n",
    "print(out)\n",
    "\n",
    "x = torch.tensor([1, 2, 3])  # shape: (3,)\n",
    "out = x.repeat((2, 1)) # .repeat() 可以接受元组参数指定每个维度的重复次数\n",
    "#                          当参数维度 > 原始维度时，会自动在左侧补充维度（本例中补充了第0维）原始张量(3,), 自动调整为(1, 3)\n",
    "#                          最终输出是二维张量，包含两行相同的 [1, 2, 3]\n",
    "#                          这种用法常用于需要将一维数据扩展为二维矩阵的场景（如批处理操作）\n",
    "print(\"(2,1)\", out)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===== torch.repeat_interleave\n",
    "x = torch.tensor([1, 2, 3])\n",
    "out = torch.repeat_interleave(x, 2)\n",
    "print(out)\n",
    "# 输出: tensor([1, 1, 2, 2, 3, 3])\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]]) # shape: (2, 2)\n",
    "out = torch.repeat_interleave(x, 2, dim=0)\n",
    "print(out)\n",
    "# 输出:\n",
    "# tensor([[1, 2],\n",
    "#         [1, 2],\n",
    "#         [3, 4],\n",
    "#         [3, 4]])\n",
    "\n",
    "x = torch.tensor([10, 20, 30])\n",
    "repeats = torch.tensor([1, 2, 3])\n",
    "out = torch.repeat_interleave(x, repeats)\n",
    "print(out)\n",
    "# 输出: tensor([10, 20, 20, 30, 30, 30])\n",
    "\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "# shape: (2, 2)\n",
    "out = torch.repeat_interleave(x, repeats=2, dim=1)\n",
    "print(out)\n",
    "# 输出:\n",
    "# tensor([[1, 1, 2, 2],\n",
    "#         [3, 3, 4, 4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch mask\n",
    "tensor[mask] , tensor 沿着 mask（掩码）取值，也叫 布尔索引（boolean indexing）,  结果是一个一维向量, tensor[number]\n",
    "mask 就是一个 布尔（bool）类型的 tensor，每个位置是 True 或 False，它和原始 tensor 形状一致或可广播，用来表示：\n",
    "\n",
    "布尔索引 tensor[mask] 时，mask.shape 必须能和 tensor.shape 完全对齐，否则就会报错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 30])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[False,  True,  True],\n",
      "        [ True, False,  True],\n",
      "        [ True,  True, False]])\n",
      "tensor([20, 30, 40, 60, 70, 80])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 过滤掉值为False的元素\n",
    "a = torch.tensor([10, 20, 30, 40])\n",
    "mask = torch.tensor([True, False, True, False])\n",
    "\n",
    "result = a[mask]\n",
    "print(result)  # tensor([10, 30])\n",
    "\n",
    "# 2. 过滤掉对角线\n",
    "eye = torch.eye(3)\n",
    "print(eye)\n",
    "# tensor([[1., 0., 0.],\n",
    "#         [0., 1., 0.],\n",
    "#         [0., 0., 1.]])\n",
    "\n",
    "mask = (1 - eye).bool()\n",
    "print(mask)\n",
    "# tensor([[False,  True,  True],\n",
    "#         [ True, False,  True],\n",
    "#         [ True,  True, False]])\n",
    "\n",
    "a = torch.tensor([[10, 20, 30],\n",
    "                  [40, 50, 60],\n",
    "                  [70, 80, 90]])\n",
    "\n",
    "result = a[mask]\n",
    "print(result)  # tensor([20, 30, 40, 60, 70, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 常见表示和符号总结\n",
    "\n",
    "行向量：\n",
    "* 在Pytorch官网文档符号上，[Liner](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) $x$ 、[RNN](https://docs.pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN) 的 $x_t$\n",
    "* 在Pytorch python输出上，torch.random(3)一般也是行向量。\n",
    "* 在《动手深度学习》里面，$x^i$ 一般理解为行向量\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
